---

## Model validation

- One commonly used model validation technique is to partition your data into training
and testing set

- That is, fit the model on the training data

- And test it on the testing data

- Evaluate model performance using $RMSE$, root-mean squared error

$$ RMSE = \sqrt{\frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{n}} $$

.question[
Do you think we should prefer low or high RMSE?
]

---

## Random sampling and reproducibility

Gotta set a seed!
```{r}
set.seed(3518)
```

- Use different seeds from each other

- Need inspiration? https://www.random.org/

---

## Cross validation

More specifically, **k-fold cross validation**:

- Split your data into k folds.

- Use 1 fold for testing and the remaining (k - 1) folds for training.

- Repeat k times.

---

## Aside -- the modulo operator

```{r}
9 %% 5
```

--

.pull-left[
```{r echo=FALSE}
df <- tibble(obs = 1:8, fold = as.integer(c(1,2,3,4,5,1,2,3)))
df %>% datatable(options = list(dom = "t"), rownames = FALSE)
```
]

--

.pull-right[
```{r}
(1:8) %% 5
((1:8) - 1) %% 5
(((1:8) - 1) %% 5) + 1
```
]

---

## Prepping your data for 5-fold CV

```{r}
evals_cv <- evals %>%
  mutate(id = 1:n()) %>%
  sample_n(nrow(evals)) %>%
  mutate( fold = (((1:n()) - 1) %% 5) + 1 )

evals_cv %>% 
  count(fold)
```

---

## CV 1

```{r}
test_fold <- 1
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% anti_join(test, by = "id")
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
(rmse_test1 <- rmse(mod, test))
```

---

## RMSE on training vs. testing

.question[
Would you expect the RMSE to be higher for your training data or your testing data? Why?
]

---

## RMSE on training vs. testing

RMSE for testing:
.small[
```{r}
(rmse_test1 <- rmse(mod, test))
```
]

RMSE for training:
.small[
```{r}
(rmse_train1 <- rmse(mod, train))
```
]

---

## CV 2

```{r}
test_fold <- 2
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% anti_join(test, by = "id")
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
```

```{r}
(rmse_test2 <- rmse(mod, test))
(rmse_train2 <- rmse(mod, train))
```

---

## CV 3

```{r}
test_fold <- 3
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% anti_join(test, by = "id")
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
```

```{r}
(rmse_test3 <- rmse(mod, test))
(rmse_train3 <- rmse(mod, train))
```

---

## CV 4

```{r}
test_fold <- 4
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% anti_join(test, by = "id")
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
```

```{r}
(rmse_test4 <- rmse(mod, test))
(rmse_train4 <- rmse(mod, train))
```

---

## CV 5

```{r}
test_fold <- 5
test <- evals_cv %>% filter(fold == test_fold)
train <- evals_cv %>% anti_join(test, by = "id")
mod <- lm(score ~ ethnicity + gender + language + age + cls_perc_eval + 
    cls_credits + bty_avg, data = train)
```

```{r}
(rmse_test5 <- rmse(mod, test))
(rmse_train5 <- rmse(mod, train))
```

---

## Putting it altogether

.small[
```{r}
rmse_evals <- tibble(
  test_fold  = 1:5,
  rmse_train = c(rmse_train1, rmse_train2, rmse_train3, rmse_train4, rmse_train5),
  rmse_test  = c(rmse_test1, rmse_test2, rmse_test3, rmse_test4, rmse_test5)
)
```

```{r fig.height=2}
ggplot(data = rmse_evals, mapping = aes(x = test_fold, y = rmse_test)) +
  geom_point() +
  geom_line()
```
]

---

## How does RMSE compare to y?

- `score` summary stats:

```{r echo=FALSE}
evals %>%
  summarise(min = min(score), max = max(score), 
            mean = mean(score), med = median(score),
            sd = sd(score), IQR = IQR(score))
```

- `rmse_test` summary stats:

```{r echo=FALSE}
rmse_evals %>%
  summarise(min = min(rmse_test), max = max(rmse_test), 
            mean = mean(rmse_test), med = median(rmse_test),
            sd = sd(rmse_test), IQR = IQR(rmse_test))
```
