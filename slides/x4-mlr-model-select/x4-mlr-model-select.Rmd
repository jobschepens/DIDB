---
title: "Multiple linear regression + Model selection"
output:
  xaringan::moon_reader:
    css: "../slides.css"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)
library(plotly)
library(widgetframe)
```

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warn = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 2.5, fig.width = 5, dpi = 300) 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}
# For ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```

class: center, middle

## Data cleaning

---

## Load data

```{r message=FALSE}
pp <- read_csv("../../data/paris-paintings.csv", 
               na = c("n/a", "", "NA"))
```

---

## Shape and material

Collapse levels of `Shape` and `mat`erial variables with `forcats::fct_collapse`:

.small[
```{r}
pp <- pp %>%
  mutate(
    Shape = fct_collapse(Shape, oval = c("oval", "ovale"),
                                round = c("round", "ronde"),
                                squ_rect = "squ_rect",
                                other = c("octogon", "octagon", "miniature")),
    mat = fct_collapse(mat, metal = c("a", "br", "c"),
                            canvas = c("co", "t", "ta"),
                            paper = c("p", "ca"),
                            wood = "b",
                            other = c("e", "g", "h", "mi", "o", "pa", "v", "al", "ar", "m"))
  )
```
]

---

## Review fixes

.pull-left[
```{r}
pp %>%
  count(Shape)
```
]

.pull-right[
```{r}
pp %>%
  count(mat)
```
]

---

## Main effects, numerical predictors

```{r}
m_main_n <- lm(log(price) ~ Width_in + Height_in, data = pp)
tidy(m_main_n) %>%
  select(term, estimate) %>%
  mutate(estimate = round(estimate, 3))
```

---

## Visualizing the model

```{r plotly, echo=FALSE, warning=FALSE, cache=F}
p <- plot_ly(pp, x = ~Width_in, y = ~Height_in, z = ~log(price),
        marker = list(size = 3,
                       color = "lightgray",
                       alpha = 0.5,
                       line = list(color = "gray",
                                   width = 2))) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = "Width (in)"),
                      yaxis = list(title = "Height (in)"),
                      zaxis = list(title = "Log(price)"))) %>%
  config(displayModeBar = FALSE)

frameWidget(p, width = '100%', height = '100%')
```

---

---


--- 

## Price, surface area, and living artist

- Explore the relationship between price of paintings and surface area, conditioned 
on whether or not the artist is still living
- First visualize and explore, then model
- But first, prep the data

.midi[
```{r test}
pp <- pp %>%
  mutate(artistliving = if_else(artistliving == 0, "Deceased", "Living"))

pp %>%
  count(artistliving)
```
]

---

--- 

## Typical surface area

.question[
What is the typical surface area for paintings?
]

```{r viz-surf-artistliving, echo=FALSE,warning=FALSE, fig.width=5, fig.height=2}
ggplot(data = pp, 
       mapping = aes(x = Surface, fill = artistliving)) +
  geom_histogram(binwidth = 500) + 
  facet_grid(artistliving ~ .) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(y = "") +
  geom_vline(xintercept = 1000) +
  geom_vline(xintercept = 5000, linetype = "dashed", color = "gray")
```

--

Less than 1000 square inches (~ 80cm x 80cm). There are very few paintings that have surface area above 5000 square inches.


---

## Main effects, numerical and categorical predictors

.small[
```{r}
pp_Surf_lt_5000 <- pp %>% filter(Surface < 5000)
m_main <- lm(log(price) ~ Surface + factor(artistliving), data = pp_Surf_lt_5000)
tidy(m_main) %>%
  select(term, estimate) %>%
  mutate(exp_estimate = round(exp(estimate), 4))
```

```{r include=FALSE}
m_main_coefs <- tidy(m_main) %>% 
  mutate(exp_estimate = round(exp(estimate), 4)) %>%
  select(term, exp_estimate)
```

- Paintings that are by an artist who is not alive and that have a surface area of 0 square inches are predicted, on average, to be `r m_main_coefs %>% filter(term == "(Intercept)") %>% pull(exp_estimate)` livres.
- For each additional square inch in painting's surface area, the price of the painting is predicted, on average, to be higher by a factor of `r m_main_coefs %>% filter(term == "Surface") %>% pull(exp_estimate)`.
- Paintings by a living artist are predicted, on average, to be higher by a factor of `r m_main_coefs %>% filter(term == "factor(artistliving)Living") %>% pull(exp_estimate)` compared to paintings by an artist who is no longer alive.

]


---

## Facet to get a better look

```{r viz-surf-lt-5000-artistliving-facet, echo=FALSE, warning=FALSE, fig.width=5, fig.height=3}
ggplot(data = pp_Surf_lt_5000, 
       mapping = aes(y = log(price), x = Surface, 
                     color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  scale_x_continuous(breaks = c(1000, 5000)) + 
  facet_wrap(~factor(artistliving)) +
  labs(color = "Living artist") +
  theme_minimal()
```

---

## What went wrong?

.question[
Why is our linear regression model (right) different from what we get from `geom_smooth(method = "lm")` (left)?
]

.pull-left[
```{r echo=FALSE, fig.height=4, warning=F}
ggplot(pp_Surf_lt_5000, aes(x = Surface, y = log(price), color = factor(artistliving))) + 
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", formula = "y ~ x") +
  labs(x = "Surface", y = "Log(price)", color = "Living artist")
```
]
.pull-right[
```{r echo=FALSE, fig.height=4}
m_pr <- lm(log(price) ~ Surface + factor(artistliving), data = pp_Surf_lt_5000)
m_pr_aug <- augment(m_pr)

ggplot(data = m_pr_aug, mapping = aes(y = log.price., x = Surface, color = factor.artistliving.)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(y = .fitted))
```
]

---

## What went wrong? (cont.) 

- The way we specified our model only lets `artistliving` affect the intercept.
- Model implicitly assumes that paintings with living and deceased artists have the *same slope* and only allows for *different intercepts*.  
- What seems more appropriate in this case? 
    * Same slope and same intercept for both colors
    * Same slope and different intercept for both colors
    * Different slope and different intercept for both colors?

---

## Interacting explanatory variables

- Including an interaction effect in the model allows for different slopes, i.e. 
nonparallel lines.
- This implies that the regression coefficient for an explanatory variable would 
change as another explanatory variable changes.
- This can be accomplished by adding an interaction variable: the product of two 
explanatory variables.

---

## Interaction: surface * artist living

.small[
```{r fig.height=2.5}
ggplot(data = pp_Surf_lt_5000,
       mapping = aes(y = log(price), x = Surface, 
                     color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", formula = "y ~ x") +
  labs(x = "Surface", y = "Log(price)", color = "Living artist")
```
]

---

## Modeling with interaction effects

.small[
```{r}
m_int <- lm(log(price) ~ Surface + factor(artistliving) +
              Surface * factor(artistliving), 
            data = pp_Surf_lt_5000)
tidy(m_int) %>% 
  select(term, estimate)
```
]

$$ \widehat{log(price)} = 4.91 + 0.00021~surface - 0.126~artistliving $$
$$+ ~ 0.00048~surface \times artistliving $$

---

## Interpretation of interaction effects

- Rate of change in price as the surface area of the painting increases does 
vary between paintings by living and non-living artists (different slopes), 
- Some paintings by living artists are more expensive than paintings by
non-living artists, and some are not (different intercept).

.small[
.pull-left[
- Non-living artist:
$\widehat{log(price)} = 4.91 + 0.00021~surface$
$- 0.126 \times 0 + 0.00048~surface \times 0$
$= 4.91 + 0.00021~surface$
- Living artist: 
$\widehat{log(price)} = 4.91 + 0.00021~surface$
$- 0.126 \times 1 + 0.00048~surface \times 1$
$= 4.91 + 0.00021~surface$
$- 0.126 + 0.00048~surface$
$= 4.784 + 0.00069~surface$
]
.pull-right[
```{r fig.height=4, echo = FALSE}
ggplot(data = pp_Surf_lt_5000,
       aes(y = log(price), x = Surface, color = factor(artistliving))) +
  geom_point(alpha = 0.3) +
  stat_smooth(method = "lm", formula = "y ~ x", fullrange = TRUE)
```
]
]

---

## Third order interactions

- Can you? Yes
- Should you? Probably not if you want to interpret these interactions in context
of the data.


---

class: center, middle

# Quality of fit in MLR

---

## $R^2$

- $R^2$ is the percentage of variability in the response variable explained by the 
regression model.

```{r}
glance(m_main)$r.squared
glance(m_int)$r.squared
```

--
- Clearly the model with interactions has a higher $R^2$.
--
- However using $R^2$ for model selection in models with multiple explanatory 
variables is not a good idea as $R^2$ increases when **any** variable is added to the 
model.
---

## $R^2$ - first principles

$$ R^2 =  \frac{ SS\_{Reg} }{ SS\_{Total} } = 1 - \left( \frac{ SS\_{Error} }{ SS\_{Total} } \right) $$

.question[
$R^2$ is calculated based on the output below.
]

```{r digits = 3}
anova(m_main)
```

---

## Adjusted $R^2$

$$ R^2\_{adj} = 1 - \left( \frac{ SS\_{Error} }{ SS\_{Total} } \times \frac{n - 1}{n - k - 1} \right), $$

where $n$ is the number of cases and $k$ is the number of predictors in the model

--
- Adjusted $R^2$ doesn't increase if the new variable does not provide any new 
informaton or is completely unrelated.
--
- This makes adjusted $R^2$ a preferable metric for model selection in multiple
regression models.
---

## In pursuit of Occam's Razor
- Occam's Razor states that among competing hypotheses that predict equally well, 
the one with the fewest assumptions should be selected.
- Model selection follows this principle.
- We only want to add another variable to the model if the addition of that
variable brings something valuable in terms of predictive power to the model.
- In other words, we prefer the simplest best model, i.e. **parsimonious** model.

---

## Comparing models

It appears that adding the interaction actually increased adjusted $R^2$, so we 
should indeed use the model with the interactions.

```{r}
glance(m_main)$adj.r.squared
glance(m_int)$adj.r.squared
```

---

class: center, middle

# Model selection

---

## Backwards elimination

- Start with **full** model (including all candidate explanatory variables and all
candidate interactions)
- Remove one variable at a time, and select the model with the highest adjusted $R^2$
- Continue until adjusted $R^2$ does not increase

---

## Forward selection

- Start with **empty** model
- Add one variable (or interaction effect) at a time, and select the model with the 
highest adjusted $R^2$
- Continue until adjusted $R^2$ does not increase

---

## Model selection and interaction effects

If an interaction is included in the model, the main effects of both of
those variables must also be in the model

If a main effect is not in the model, then its interaction should not be 
in the model.

---

## Other model selection criteria

- Adjusted $R^2$ is one model selection criterion
- There are others out there (many many others!), we'll discuss some later in the 
course, and some you might see in another courses

---